Overview
--------

The starting point for any nextflow.py pipeline is the :py:class:`.Pipeline`
object. This is initialised with a path to the file in question, and,
optionally, the location of an accompanying config file:

    >>> pipeline1 = nextflow.Pipeline("pipelines/my-pipeline.nf")
    >>> pipeline2 = nextflow.Pipeline("main.nf", config="nextflow.config")

Running
~~~~~~~

To actually execute the pipeline, the :py:meth:`.run` method is used:

    >>> execution = pipeline.run()

This will return an :py:class:`.Execution` object, which represents the pipeline
execution that just took place. You can customise the execution with various
options:

    >>> execution = pipeline.run(location="./rundir", params={"param1": "123"}, profile=["docker", "test"], version="22.0.1", config=["env.config"])

This sets the execution to take place in a different location, passes
``--param1=123`` as a command line argument when the pipeline is run, uses the
Nextflow profiles 'docker' and 'test', runs with Nextflow version 22.0.1
(regardless of what version of Nextflow is installed), and passes in an extra
config file to use on the run.

Executions
##########

An :py:class:`.Execution` represents a single execution of a
:py:class:`.Pipeline`. It has properties for:

* ``id`` - The unique ID of that run, generated by Nextflow.

* ``started`` - When the pipeline ran (as a UNIX timestamp).

* ``started_dt`` - When the pipeline ran (as a Python datetime).

* ``duration`` - how long the execution took in seconds.

* ``status`` - the status Nextflow reports on completion.

* ``command`` - the command used to run the pipeline.

* ``stdout`` - the stdout of the execution process.

* ``stderr`` - the stderr of the execution process.

* ``log`` - the full text of the log file produced.

* ``returncode`` - the exit code of the run - usually 0 or 1.

* ``pipeline`` - the :py:class:`.Pipeline` that created the execution.

It also has a ``process_executions`` property, which is a list of
:py:class:`.ProcessExecution` objects. Nextflow processes data by chaining
together isolated 'processes', and each of these has a
:py:class:`.ProcessExecution` object representing its execution. These have the
following properties:

* ``hash`` - The unique ID generated by Nextflow, of the form ``xx/xxxxxx``.

* ``process`` - The name of the process that spawned the process execution.

* ``name`` - The name of this specific process execution.

* ``status`` - the status Nextflow reports on completion.

* ``stdout`` - the stdout of the process execution.

* ``stderr`` - the stderr of the process execution.

* ``started`` - When the process execution ran (as a UNIX timestamp).

* ``started_dt`` - When the process execution ran (as a Python datetime).

* ``duration`` - how long the process execution took in seconds.

* ``returncode`` - the exit code of the process execution - usually 0 or 1.

Process executions can have various files passed to them, and will create files
during their execution too. These can be obtained as follows:

    >>> process_execution.input_data() # Full absolute paths
    >>> process_execution.input_data(include_path=False) # Just file names
    >>> process_execution.all_output_data() # Full absolute paths
    >>> process_execution.all_output_data(include_path=False) # Just file names

.. note::
   Nextflow makes a distinction between process output files which were
   'published' via some channel, and those which weren't. It is not possible to
   distinguish these once execution is complete, so nextflow.py reports all
   output files, not just those which are 'published'.

Polling
~~~~~~~

The method described above will run the pipeline and wait while it does, with
the completed :py:class:`.Execution` being returned only at the end.

An alternate method is to use :py:meth:`.run_and_poll`, which returns an
:py:class:`.Execution` object every few seconds representing the state of the
pipeline execution at that moment in time, as a generator::

    for execution in pipeline.run_and_poll(sleep=2, location="./rundir", params={"param1": "123"}, profile=["docker", "test"], version="22.0.1"):
        print("Processing intermediate execution")

By default, an :py:class:`.Execution` will be returned every 5 seconds, but you
can adjust this as required with the ``sleep`` paramater. This is useful if you
want to get information about the progress of the pipeline execution as it
proceeds.

Direct Running
~~~~~~~~~~~~~~

If you just want to run a single pipeline without initialising a
:py:class:`.Pipeline` object first, you can :py:func:`.run` or
:py:func:`.run_and_poll` directly, without needing to create a
:py:class:`.Pipeline`:

    >>> import nextflow
    >>> execution = nextflow.run(path="pipeline.nf", config=["settings.config"], params={"param1": "123"})